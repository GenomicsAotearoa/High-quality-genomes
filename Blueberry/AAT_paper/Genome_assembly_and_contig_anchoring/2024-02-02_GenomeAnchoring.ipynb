{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16798741-bc3a-4966-803c-221ab911f6d0",
   "metadata": {},
   "source": [
    "# Genome anchoring using synteny and linkage map\n",
    "\n",
    "* ms figure 1. Distribution of SNP markers from the phased parental linkage map (Montanari et al., 2022) on the four V06.A002-186 contig sets of chromosome 2.\n",
    "  \n",
    "## 1. mapping haplotype contigs to reference genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a135eac3-f8e9-471f-9ac7-a9ef9c37254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WDR=002.phasingFromGeneticMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c7cec04-f767-4ca0-8965-ac42eae09316",
   "metadata": {},
   "outputs": [],
   "source": [
    "REF=VcaeV1.3.p0.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72f306b9-9bb6-4924-b5c6-c9fb312cda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml samtools/1.16\n",
    "samtools faidx $REF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666d5cf8-6d97-4f93-8a06-82e3bb6f4782",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAPS=/workspace/hraijc/Blueberry/Blueberry_trio/Assembly/postcontamremoval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "503bdd7c-1d0e-4a6e-a58b-be75e5eefc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3709745\n",
      "Submitted batch job 3709746\n",
      "Submitted batch job 3709747\n",
      "Submitted batch job 3709748\n"
     ]
    }
   ],
   "source": [
    "CONTIG_SETS=\"classified_M7_plus_unclassified_hap1 classified_M7_plus_unclassified_hap2 classified_Nui_plus_unclassified_hap1 classified_Nui_plus_unclassified_hap2\"\n",
    "\n",
    "for i in $CONTIG_SETS; do\n",
    "\n",
    "sbatch << EOF\n",
    "#!/bin/bash\n",
    "#SBATCH -J mapping\n",
    "#SBATCH -o ${WDR}/mapping_per_hap.out\n",
    "#SBATCH -e ${WDR}/mapping_per_hap.err\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=20G\n",
    "#SBATCH --time=10:00:00\n",
    "\n",
    "module load minimap2/2.22\n",
    "ml samtools/1.16\n",
    "\n",
    "minimap2 -t 16 -ax asm10 $REF $HAPS/$i.clean.fa > $WDR/$i.clean.sam\n",
    "samtools view -@ 16 -b $WDR/$i.clean.sam > $WDR/$i.clean.bam\n",
    "samtools sort -@ 16 $WDR/$i.clean.bam > $WDR/$i.clean.sorted.bam\n",
    "samtools index -@ 16 $WDR/$i.clean.sorted.bam\n",
    "\n",
    "EOF\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "654e0f82-5cb4-4ff7-ae15-08dd65719636",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "About:   SNP/indel variant calling from VCF/BCF. To be used in conjunction with bcftools mpileup.\n",
      "         This command replaces the former \"bcftools view\" caller. Some of the original\n",
      "         functionality has been temporarily lost in the process of transition to htslib,\n",
      "         but will be added back on popular demand. The original calling model can be\n",
      "         invoked with the -c option.\n",
      "Usage:   bcftools call [options] <in.vcf.gz>\n",
      "\n",
      "File format options:\n",
      "       --no-version                Do not append version and command line to the header\n",
      "   -o, --output FILE               Write output to a file [standard output]\n",
      "   -O, --output-type b|u|z|v       Output type: 'b' compressed BCF; 'u' uncompressed BCF; 'z' compressed VCF; 'v' uncompressed VCF [v]\n",
      "   -O, --output-type u|b|v|z[0-9]  u/b: un/compressed BCF, v/z: un/compressed VCF, 0-9: compression level [v]\n",
      "       --ploidy ASSEMBLY[?]        Predefined ploidy, 'list' to print available settings, append '?' for details [2]\n",
      "       --ploidy-file FILE          Space/tab-delimited list of CHROM,FROM,TO,SEX,PLOIDY\n",
      "   -r, --regions REGION            Restrict to comma-separated list of regions\n",
      "   -R, --regions-file FILE         Restrict to regions listed in a file\n",
      "       --regions-overlap 0|1|2     Include if POS in the region (0), record overlaps (1), variant overlaps (2) [1]\n",
      "   -s, --samples LIST              List of samples to include [all samples]\n",
      "   -S, --samples-file FILE         PED file or a file with an optional column with sex (see man page for details) [all samples]\n",
      "   -t, --targets REGION            Similar to -r but streams rather than index-jumps\n",
      "   -T, --targets-file FILE         Similar to -R but streams rather than index-jumps\n",
      "       --threads INT               Use multithreading with INT worker threads [0]\n",
      "\n",
      "Input/output options:\n",
      "   -A, --keep-alts                 Keep all possible alternate alleles at variant sites\n",
      "   -a, --annotate LIST             Optional tags to output (lowercase allowed); '?' to list available tags\n",
      "   -F, --prior-freqs AN,AC         Use prior allele frequencies, determined from these pre-filled tags\n",
      "   -G, --group-samples FILE|-      Group samples by population (file with \"sample\\tgroup\") or \"-\" for single-sample calling.\n",
      "                                   This requires FORMAT/QS or other Number=R,Type=Integer tag such as FORMAT/AD\n",
      "       --group-samples-tag TAG     The tag to use with -G, by default FORMAT/QS and FORMAT/AD are checked automatically\n",
      "   -g, --gvcf INT,[...]            Group non-variant sites into gVCF blocks by minimum per-sample DP\n",
      "   -i, --insert-missed             Output also sites missed by mpileup but present in -T\n",
      "   -M, --keep-masked-ref           Keep sites with masked reference allele (REF=N)\n",
      "   -V, --skip-variants TYPE        Skip indels/snps\n",
      "   -v, --variants-only             Output variant sites only\n",
      "\n",
      "Consensus/variant calling options:\n",
      "   -c, --consensus-caller          The original calling method (conflicts with -m)\n",
      "   -C, --constrain STR             One of: alleles, trio (see manual)\n",
      "   -m, --multiallelic-caller       Alternative model for multiallelic and rare-variant calling (conflicts with -c)\n",
      "   -n, --novel-rate FLOAT,[...]    Likelihood of novel mutation for constrained trio calling, see man page for details [1e-8,1e-9,1e-9]\n",
      "   -p, --pval-threshold FLOAT      Variant if P(ref|D)<FLOAT with -c [0.5]\n",
      "   -P, --prior FLOAT               Mutation rate (use bigger for greater sensitivity), use with -m [1.1e-3]\n",
      "\n",
      "Example:\n",
      "   # See also http://samtools.github.io/bcftools/howtos/variant-calling.html\n",
      "   bcftools mpileup -Ou -f reference.fa alignments.bam | bcftools call -mv -Ob -o calls.bcf\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "255",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "ml bcftools/1.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf4ad3c9-b211-4ae3-aba6-28dad7ee2574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3772207\n",
      "Submitted batch job 3772208\n",
      "Submitted batch job 3772209\n",
      "Submitted batch job 3772210\n"
     ]
    }
   ],
   "source": [
    "## variant calling from samtools\n",
    "\n",
    "CONTIG_SETS=\"classified_M7_plus_unclassified_hap1 classified_M7_plus_unclassified_hap2 classified_Nui_plus_unclassified_hap1 classified_Nui_plus_unclassified_hap2\"\n",
    "\n",
    "for i in $CONTIG_SETS; do\n",
    "\n",
    "sbatch << EOF\n",
    "#!/bin/bash\n",
    "#SBATCH -J calling\n",
    "#SBATCH -o ${WDR}/calling_per_hap.allcalls.out\n",
    "#SBATCH -e ${WDR}/calling_per_hap.allcalls.err\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=20G\n",
    "#SBATCH --time=10:00:00\n",
    "\n",
    "ml bcftools/1.16\n",
    "\n",
    "bcftools mpileup -Ou -f $WDR/VcaeV1.3.p0.fa $WDR/$i.clean.sorted.bam | bcftools call --ploidy 1 -V -c -Ov -o $WDR/$i.clean.allcalls.sorted.vcf\n",
    "\n",
    "EOF\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f48f86a-a604-402d-8e11-c1f4a58c1580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3710041\n",
      "Submitted batch job 3710042\n",
      "Submitted batch job 3710043\n",
      "Submitted batch job 3710044\n"
     ]
    }
   ],
   "source": [
    "CONTIG_SETS=\"classified_M7_plus_unclassified_hap1 classified_M7_plus_unclassified_hap2 classified_Nui_plus_unclassified_hap1 classified_Nui_plus_unclassified_hap2\"\n",
    "\n",
    "for i in $CONTIG_SETS; do\n",
    "\n",
    "sbatch << EOF\n",
    "#!/bin/bash\n",
    "#SBATCH -J calling\n",
    "#SBATCH -o ${WDR}/calling_bgzip.out\n",
    "#SBATCH -e ${WDR}/calling_bgzip.err\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=20G\n",
    "#SBATCH --time=10:00:00\n",
    "\n",
    "ml bcftools/1.16\n",
    "\n",
    "bcftools view $WDR/$i.clean.sorted.vcf -Oz -o $WDR/$i.clean.sorted.vcf.gz\n",
    "bcftools index $WDR/$i.clean.sorted.vcf.gz\n",
    "\n",
    "EOF\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d798a8b9-f1ab-4b16-a140-f26acea54913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3710045\n"
     ]
    }
   ],
   "source": [
    "sbatch << EOF\n",
    "#!/bin/bash\n",
    "#SBATCH -J calling\n",
    "#SBATCH -o ${WDR}/calling_merge.out\n",
    "#SBATCH -e ${WDR}/calling_merge.err\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=20G\n",
    "#SBATCH --time=10:00:00\n",
    "\n",
    "ml bcftools/1.16\n",
    "\n",
    "bcftools merge -Ov $WDR/classified_M7_plus_unclassified_hap1.clean.sorted.vcf.gz $WDR/classified_M7_plus_unclassified_hap2.clean.sorted.vcf.gz $WDR/classified_Nui_plus_unclassified_hap1.clean.sorted.vcf.gz $WDR/classified_Nui_plus_unclassified_hap2.clean.sorted.vcf.gz > $WDR/merged.vcf\n",
    "\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779cf2e-6107-4f6d-b0c9-d1b08694738b",
   "metadata": {},
   "source": [
    "## 2. extract variants called from genetic map positions from merged.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d561edc9-23ee-40f9-95c8-ab87a6bfd8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq-0-002_1002949\n",
      "seq-0-002_10212141\n",
      "seq-0-002_10212213\n",
      "seq-0-002_10212217\n",
      "seq-0-002_10248216\n",
      "seq-0-002_10260806\n",
      "seq-0-002_10264525\n",
      "seq-0-002_10328926\n",
      "seq-0-002_10348643\n",
      "seq-0-002_10348661\n"
     ]
    }
   ],
   "source": [
    "# genetic map positions obtained from previous study: \"Montanari S, Thomson S, Cordiner S, Günther CS, Miller P, Deng CH, McGhie T, Knäbel M, Foster T, Turner J, Chagné D, Espley R. High-density linkage map construction in an autotetraploid blueberry population and detection of quantitative trait loci for anthocyanin content. Front Plant Sci. 2022 Sep 23;13:965397. doi: 10.3389/fpls.2022.965397. PMID: 36247546; PMCID: PMC9555082.\"\n",
    "\n",
    "head $WDR/geneticMapPositions.chr2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04da9fdd-2f3c-4517-9ac8-8e7b27c7826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml pfr-python3/3.9.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "451db895-4ef0-42c8-abe9-f4a4a6bae615",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with open('geneticMapPositions.chr1.txt','r') as f:\n",
      "\tpos_list = [line.strip() for line in f]\n",
      "\n",
      "with open('merged.vcf', 'r') as f_vcf:\n",
      "\theader_lines = sum(1 for line in f_vcf if line.startswith('#'))\n",
      "\tf_vcf.seek(0)  # Return to the beginning of the file\n",
      "\tvcf_lines = [line.strip().split('\\t') for line in f_vcf if '#' not in line]\n",
      "\n",
      "poscalls_list = [f'{line[0]}_{line[1]}' for line in vcf_lines]\n",
      "geno_lists = [[line[i] for line in vcf_lines] for i in range(9, 13)]\n",
      "\n",
      "with open('progeny_phasingTable.txt', 'w') as f_table:\n",
      "    for pos in pos_list:\n",
      "        if pos in poscalls_list:\n",
      "            indexcalls = poscalls_list.index(pos)\n",
      "            geno_values = '\\t'.join(geno_lists[i][indexcalls] for i in range(4))\n",
      "            f_table.write(f'{pos}\\t{geno_values}\\n')\n",
      "        else:\n",
      "            f_table.write(f'{pos}\\tNaN\\tNaN\\tNaN\\tNaN\\n')\n"
     ]
    }
   ],
   "source": [
    "python << EOF\n",
    "\n",
    "with open('geneticMapPositions.chr2.txt','r') as f:\n",
    "\tpos_list = [line.strip() for line in f]\n",
    "\n",
    "with open('merged.vcf', 'r') as f_vcf:\n",
    "\theader_lines = sum(1 for line in f_vcf if line.startswith('#'))\n",
    "\tf_vcf.seek(0)  # Return to the beginning of the file\n",
    "\tvcf_lines = [line.strip().split('\\t') for line in f_vcf if '#' not in line]\n",
    "\n",
    "poscalls_list = [f'{line[0]}_{line[1]}' for line in vcf_lines]\n",
    "geno_lists = [[line[i] for line in vcf_lines] for i in range(9, 13)]\n",
    "\n",
    "with open('progeny_phasingTable.chr2.txt', 'w') as f_table:\n",
    "    for pos in pos_list:\n",
    "        if pos in poscalls_list:\n",
    "            indexcalls = poscalls_list.index(pos)\n",
    "            geno_values = '\\t'.join(geno_lists[i][indexcalls] for i in range(4))\n",
    "            f_table.write(f'{pos}\\t{geno_values}\\n')\n",
    "        else:\n",
    "            f_table.write(f'{pos}\\tNaN\\tNaN\\tNaN\\tNaN\\n')\n",
    "\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f697de-278e-4871-a66a-e3bd7245600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 4256342\n"
     ]
    }
   ],
   "source": [
    "# submit script to cluster as vcf is large, it takes long time to run\n",
    "\n",
    "sbatch << EOF\n",
    "#!/bin/bash\n",
    "#SBATCH -J tableGenerate\n",
    "#SBATCH -o ${WDR}/table_generate.chr2.out\n",
    "#SBATCH -e ${WDR}/table_generate.chr2.err\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=2G\n",
    "#SBATCH --time=1:00:00\n",
    "\n",
    "ml pfr-python3\n",
    "\n",
    "cd $WDR\n",
    "\n",
    "python phasingTableFromProgeny.py\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69077b06-f822-4609-8db4-88ce0d28bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq-0-002_1002949\t.:.\t1:60,0\t.:.\t1:60,0\n",
      "seq-0-002_10212141\tNaN\tNaN\tNaN\tNaN\n",
      "seq-0-002_10212213\t1:60,0\t1:60,0\t.:.\t.:.\n",
      "seq-0-002_10212217\t.:.\t.:.\t1:60,0\t1:60,0\n",
      "seq-0-002_10248216\tNaN\tNaN\tNaN\tNaN\n",
      "seq-0-002_10260806\tNaN\tNaN\tNaN\tNaN\n",
      "seq-0-002_10264525\tNaN\tNaN\tNaN\tNaN\n",
      "seq-0-002_10328926\t1:60,0\t.:.\t.:.\t.:.\n",
      "seq-0-002_10348643\tNaN\tNaN\tNaN\tNaN\n",
      "seq-0-002_10348661\t1:60,0\t.:.\t.:.\t.:.\n"
     ]
    }
   ],
   "source": [
    "# four columns are M7_hA, M7_hB, Nui_hA and Nui_hB\n",
    "\n",
    "head $WDR/progeny_phasingTable.chr2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b819b56-ce82-4a1b-977c-257e2c190682",
   "metadata": {},
   "source": [
    "* '.:.' could indicate 'missing data' or 'homozygous to reference allele'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e1a40-ffe8-43aa-a484-c912ebda1c89",
   "metadata": {},
   "source": [
    "## 3. extracting contig id with variants called at genetic map positions from 'bam' files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1fd3e5f-f322-495a-856a-a50e8ed08b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\n",
      " 1) \u001b[46mpowerPlant/core\u001b[0m    5) \u001b[46mSlurm/21.08.8-2\u001b[0m      9) pfr-python3/3.9.13  \n",
      " 2) texlive/20230914   6) perlbrew/0.76       10) samtools/1.12       \n",
      " 3) pandoc/1.19.2      7) perl/5.36.0         \n",
      " 4) git/2.21.0         8) slurm-utils/latest  \n",
      "\n",
      "Key:\n",
      "\u001b[46msticky\u001b[0m  \n"
     ]
    }
   ],
   "source": [
    "ml samtools/1.12\n",
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a12299-3442-43fa-a078-636f6c0d39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $WDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2aca3-6336-4f2d-b1d9-a35c404c0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "python << EOF\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "species = ['M7', 'Nui']\n",
    "haps = ['hap1', 'hap2']\n",
    "\n",
    "for spec in species:\n",
    "\tfor hap in haps:\n",
    "\t# Assuming geneticMapPositions.txt has the chromosome and position information\n",
    "\t\twith open('geneticMapPositions.chr2.txt', 'r') as positions_file:\n",
    "\t\t\tfor line in positions_file:\n",
    "\t\t\t\tchrom, pos = line.strip().split('_')\n",
    "\t\t\t\tcommand = f\"samtools view classified_{spec}_plus_unclassified_{hap}.clean.sorted.bam {chrom}:{pos}-{pos} | cut -f1 | head -n 1\"\n",
    "\t\t\t\tresult = subprocess.run(command, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Append chrom, pos, and read IDs to 'read_ids.M7h2.txt'\n",
    "\t\t\t\twith open('mappingPositions.' + spec + '.' + hap + '.chr2.txt', 'a') as output_file:\n",
    "\t\t\t\t\tif result.stdout != '':\n",
    "\t\t\t\t\t\toutput_file.write(f\"{chrom}\\t{pos}\\t{result.stdout}\")\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\toutput_file.write(f\"{chrom}\\t{pos}\\tNaN\\n\")\n",
    "\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b410a9f-d70c-4eba-8bed-59beb1601a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq-0-002\t1002949\th1tg000119l_1\n",
      "seq-0-002\t10212141\th1tg000296l_1\n",
      "seq-0-002\t10212213\th1tg000296l_1\n",
      "seq-0-002\t10212217\th1tg000296l_1\n",
      "seq-0-002\t10248216\th1tg000296l_1\n",
      "seq-0-002\t10260806\th1tg000296l_1\n",
      "seq-0-002\t10264525\tNaN\n",
      "seq-0-002\t10328926\th1tg000296l_1\n",
      "seq-0-002\t10348643\th1tg000296l_1\n",
      "seq-0-002\t10348661\th1tg000296l_1\n"
     ]
    }
   ],
   "source": [
    "head $WDR/mappingPositions.M7.hap1.chr2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd45133-cd86-48b0-918b-2c23cc8c43bf",
   "metadata": {},
   "source": [
    "* 'NaN' means 'missing data', using this information to distinguish missing data and alleles homozygous to reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1bc142-68e9-430b-8ed0-f396df34a36a",
   "metadata": {},
   "source": [
    "* sorting out to above outputs to generate a table containing allele information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e3c1f2-e467-45cc-a055-11f3bd5b62e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq-0-002_229781\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\tNaN\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n",
      "seq-0-002_229801\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\tNaN\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n",
      "seq-0-002_229820\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\tNaN\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n",
      "seq-0-002_229830\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\tNaN\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n",
      "seq-0-002_247149\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\th1tg000119l_1\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n",
      "seq-0-002_247160\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\th1tg000119l_1\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n",
      "seq-0-002_252157\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\th1tg000119l_1\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n",
      "seq-0-002_268324\t0\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\t1\th1tg000119l_1\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n",
      "seq-0-002_297604\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\th1tg000119l_1\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n",
      "seq-0-002_299184\t0\t0\t0\t0\t1\t1\t0\t0\t1\t1\t0\t1\th1tg000119l_1\th2tg000447l_1\th1tg000203l_1\th2tg000544l_1\n"
     ]
    }
   ],
   "source": [
    "# collumn infor: M7_h1,M7_h2,M7_h3,M7_h4,Nui_h1,Nui_h2,Nui_h3,Nui_h4,M7_hA,M7_hB,Nui_hA,Nui_hB\n",
    "\n",
    "head $WDR/test/phasingFile.chr2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118868db-0608-44cb-b203-bce10543cca3",
   "metadata": {},
   "source": [
    "## 4. generate simplex alleles that inherit to progeny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a352a-abf6-48ca-bff5-45c678ea3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python << EOF\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "file_path = 'phasingFile.chr2.txt'  # Replace with the actual file path\n",
    "\n",
    "# Specify the column index (0-based) or name\n",
    "column_index = 1  # Replace with the actual column index or name\n",
    "\n",
    "# Read the specified column into a list\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [line.strip().split('\\t') for line in file]\n",
    "        marker_id = [line[0] if line[0] != '' else np.nan for line in lines]\n",
    "        M7h1_contigs_values = [line[13] if line[13] != '' else np.nan for line in lines]\n",
    "        M7h2_contigs_values = [line[14] if line[14] != '' else np.nan for line in lines]\n",
    "        M7_h1_values = [line[1] if line[1] != '' else np.nan for line in lines]\n",
    "        M7_h2_values = [line[2] if line[2] != '' else np.nan for line in lines]\n",
    "        M7_h3_values = [line[3] if line[3] != '' else np.nan for line in lines]\n",
    "        M7_h4_values = [line[4] if line[4] != '' else np.nan for line in lines]\n",
    "        M7h1_values = [line[9] if line[9] != '' else np.nan for line in lines]\n",
    "        M7h2_values = [line[10] if line[10] != '' else np.nan for line in lines]\n",
    "    M7h1_contigs_values_uniq = list(set(M7h1_contigs_values))\n",
    "    M7h2_contigs_values_uniq = list(set(M7h2_contigs_values))\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "except IndexError:\n",
    "    print(f\"Column index out of range: {column_index}\")\n",
    "\n",
    "\n",
    "for unique_value in M7h1_contigs_values_uniq:\n",
    "    matching_indices = [index for index, value in enumerate(M7h1_contigs_values) if value == unique_value]\n",
    "    print(f\"Indices of values equal to {unique_value}: {matching_indices}\")\n",
    "    corresponding_values_M7_h1 = [M7_h1_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7_h1 Values: {corresponding_values_M7_h1}\\n\")\n",
    "    corresponding_values_M7_h2 = [M7_h2_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7_h2 Values: {corresponding_values_M7_h2}\\n\")\n",
    "    corresponding_values_M7_h3 = [M7_h3_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7_h3 Values: {corresponding_values_M7_h3}\\n\")\n",
    "    corresponding_values_M7_h4 = [M7_h4_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7_h4 Values: {corresponding_values_M7_h4}\\n\")\n",
    "    corresponding_values_M7h1 = [M7h1_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7h1 Values: {corresponding_values_M7h1}\\n\")\n",
    "    corresponding_values_M7h2 = [M7h2_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7h2 Values: {corresponding_values_M7h2}\\n\")\n",
    "    \n",
    "    total_values_num = len(corresponding_values_M7h1)\n",
    "#    print(total_values_num)\n",
    "\n",
    "    print(len(corresponding_values_M7h1))\n",
    "\n",
    "    frequency_counts = Counter(corresponding_values_M7h1)\n",
    "    for value, count in frequency_counts.items():\n",
    "        print(f\"M7h1 value: {value}, Frequency: {count}\")\n",
    "\n",
    "     # Check if M7h1 value is '1' before printing corresponding values\n",
    "    for index in matching_indices:\n",
    "        output_file_path = 'test_M7h1.chr2.txt'\n",
    "        if M7h1_values[index] == '1' and M7h2_values[index] != '1':\n",
    "            print(f\"Index: {index}\")\n",
    "            corresponding_values_M7_h1 = M7_h1_values[index]\n",
    "            print(f\"Corresponding M7_h1 Value: {corresponding_values_M7_h1}\")\n",
    "            corresponding_values_M7_h2 = M7_h2_values[index]\n",
    "            print(f\"Corresponding M7_h2 Value: {corresponding_values_M7_h2}\")\n",
    "            corresponding_values_M7_h3 = M7_h3_values[index]\n",
    "            print(f\"Corresponding M7_h3 Value: {corresponding_values_M7_h3}\")\n",
    "            corresponding_values_M7_h4 = M7_h4_values[index]\n",
    "            print(f\"Corresponding M7_h4 Value: {corresponding_values_M7_h4}\")\n",
    "            corresponding_values_M7h1 = M7h1_values[index]\n",
    "            print(f\"Corresponding M7h1 Value: {corresponding_values_M7h1}\")\n",
    "            corresponding_values_M7h2 = M7h2_values[index]\n",
    "            print(f\"Corresponding M7h2 Value: {corresponding_values_M7h2}\\n\")\n",
    "            \n",
    "            corresponding_values_dict = {\n",
    "                \"corresponding_values_M7_h1\": corresponding_values_M7_h1,\n",
    "                \"corresponding_values_M7_h2\": corresponding_values_M7_h2,\n",
    "                \"corresponding_values_M7_h3\": corresponding_values_M7_h3,\n",
    "                \"corresponding_values_M7_h4\": corresponding_values_M7_h4,\n",
    "            }\n",
    "\n",
    "            print(f\"corresponding values dict: {corresponding_values_dict}\\n\")\n",
    "\n",
    "            largest_value = max(corresponding_values_dict.values())\n",
    "            print(f\"largest value: {largest_value}\\n\")\n",
    "        # Find all columns with the largest value\n",
    "            largest_columns = [column for column, variant in corresponding_values_dict.items() if variant == largest_value]\n",
    "            print(f\"largest columns: {largest_columns}\\n\")\n",
    "\n",
    "            if largest_value == '1' and len(largest_columns) < 2:\n",
    "                with open(output_file_path, 'a') as output_file:\n",
    "                    output_file.write(f\"{unique_value}\\t{marker_id[index]}\\t{largest_columns[0].split('_')[-1].split(']')[0]}\\n\")\n",
    "            \n",
    "            else:\n",
    "                with open(output_file_path, 'a') as output_file:\n",
    "                    output_file.write(f\"{unique_value}\\t{marker_id[index]}\\tNaN\\n\")\n",
    "\n",
    "        else:\n",
    "            with open(output_file_path, 'a') as output_file:\n",
    "                output_file.write(f\"{unique_value}\\t{marker_id[index]}\\tNaN\\n\")\n",
    "\n",
    "\n",
    "\n",
    "for unique_value in M7h2_contigs_values_uniq:\n",
    "    matching_indices = [index for index, value in enumerate(M7h2_contigs_values) if value == unique_value]\n",
    "    print(f\"Indices of values equal to {unique_value}: {matching_indices}\")\n",
    "    corresponding_values_M7_h1 = [M7_h1_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7_h1 Values: {corresponding_values_M7_h1}\\n\")\n",
    "    corresponding_values_M7_h2 = [M7_h2_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7_h2 Values: {corresponding_values_M7_h2}\\n\")\n",
    "    corresponding_values_M7_h3 = [M7_h3_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7_h3 Values: {corresponding_values_M7_h3}\\n\")\n",
    "    corresponding_values_M7_h4 = [M7_h4_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7_h4 Values: {corresponding_values_M7_h4}\\n\")\n",
    "    corresponding_values_M7h1 = [M7h1_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7h1 Values: {corresponding_values_M7h1}\\n\")\n",
    "    corresponding_values_M7h2 = [M7h2_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding M7h2 Values: {corresponding_values_M7h2}\\n\")\n",
    "    \n",
    "    total_values_num = len(corresponding_values_M7h2)\n",
    "\n",
    "    print(len(corresponding_values_M7h2))\n",
    "\n",
    "    frequency_counts = Counter(corresponding_values_M7h2)\n",
    "    for value, count in frequency_counts.items():\n",
    "        print(f\"M7h2 value: {value}, Frequency: {count}\")\n",
    "\n",
    "    for index in matching_indices:\n",
    "        output_file_path = 'test_M7h2.chr2.txt'\n",
    "        if M7h2_values[index] == '1' and M7h1_values[index] != '1':\n",
    "            print(f\"Index: {index}\")\n",
    "            corresponding_values_M7_h1 = M7_h1_values[index]\n",
    "            print(f\"Corresponding M7_h1 Value: {corresponding_values_M7_h1}\")\n",
    "            corresponding_values_M7_h2 = M7_h2_values[index]\n",
    "            print(f\"Corresponding M7_h2 Value: {corresponding_values_M7_h2}\")\n",
    "            corresponding_values_M7_h3 = M7_h3_values[index]\n",
    "            print(f\"Corresponding M7_h3 Value: {corresponding_values_M7_h3}\")\n",
    "            corresponding_values_M7_h4 = M7_h4_values[index]\n",
    "            print(f\"Corresponding M7_h4 Value: {corresponding_values_M7_h4}\")\n",
    "            corresponding_values_M7h1 = M7h1_values[index]\n",
    "            print(f\"Corresponding M7h1 Value: {corresponding_values_M7h1}\")\n",
    "            corresponding_values_M7h2 = M7h2_values[index]\n",
    "            print(f\"Corresponding M7h2 Value: {corresponding_values_M7h2}\\n\")\n",
    "            \n",
    "            corresponding_values_dict = {\n",
    "                \"corresponding_values_M7_h1\": corresponding_values_M7_h1,\n",
    "                \"corresponding_values_M7_h2\": corresponding_values_M7_h2,\n",
    "                \"corresponding_values_M7_h3\": corresponding_values_M7_h3,\n",
    "                \"corresponding_values_M7_h4\": corresponding_values_M7_h4,\n",
    "            }\n",
    "\n",
    "            print(f\"corresponding values dict: {corresponding_values_dict}\\n\")\n",
    "\n",
    "            largest_value = max(corresponding_values_dict.values())\n",
    "            print(f\"largest value: {largest_value}\\n\")\n",
    "        # Find all columns with the largest value\n",
    "            largest_columns = [column for column, variant in corresponding_values_dict.items() if variant == largest_value]\n",
    "            print(f\"largest columns: {largest_columns}\\n\")\n",
    "\n",
    "            if largest_value == '1' and len(largest_columns) < 2:\n",
    "                with open(output_file_path, 'a') as output_file:\n",
    "                    output_file.write(f\"{unique_value}\\t{marker_id[index]}\\t{largest_columns[0].split('_')[-1].split(']')[0]}\\n\")\n",
    "            else:\n",
    "                with open(output_file_path, 'a') as output_file:\n",
    "                    output_file.write(f\"{unique_value}\\t{marker_id[index]}\\tNaN\\n\")\n",
    "        else:\n",
    "            with open(output_file_path, 'a') as output_file:\n",
    "                output_file.write(f\"{unique_value}\\t{marker_id[index]}\\tNaN\\n\")\n",
    "\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a46cb854-8ba3-409e-9043-94c738b2c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1tg000403l_1\tseq-0-002_52803798\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52803856\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52915496\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52915504\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52915512\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52915572\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52915584\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52940536\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52944993\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52945000\tNaN\n",
      "h1tg000403l_1\tseq-0-002_52956657\tNaN\n",
      "h1tg000403l_1\tseq-0-002_53009574\tNaN\n",
      "h1tg000487l_1\tseq-0-002_66525617\th4\n",
      "h1tg000487l_1\tseq-0-002_66529476\th4\n",
      "h1tg000459l_1\tseq-0-002_27568333\th1\n",
      "h1tg000459l_1\tseq-0-002_27698231\tNaN\n",
      "h1tg000459l_1\tseq-0-002_27710359\tNaN\n",
      "h1tg000459l_1\tseq-0-002_27766344\tNaN\n",
      "h1tg000459l_1\tseq-0-002_27766380\th1\n",
      "h1tg000459l_1\tseq-0-002_27972595\tNaN\n"
     ]
    }
   ],
   "source": [
    "# h1-h4 are M7 parental genetic map\n",
    "\n",
    "head -n 20 $WDR/test/test_M7h1.chr2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38785484-da52-41e3-bf06-b75fd6690bba",
   "metadata": {},
   "source": [
    "* same script was run on Nui file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc6b6e-f98d-440d-86db-772c60c8d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "python << EOF\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "file_path = 'phasingFile.chr2.txt'  # Replace with the actual file path\n",
    "\n",
    "# Specify the column index (0-based) or name\n",
    "column_index = 1  # Replace with the actual column index or name\n",
    "\n",
    "# Read the specified column into a list\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [line.strip().split('\\t') for line in file]\n",
    "        marker_id = [line[0] if line[0] != '' else np.nan for line in lines]\n",
    "        Nuih1_contigs_values = [line[15] if line[15] != '' else np.nan for line in lines]\n",
    "        Nuih2_contigs_values = [line[16] if line[16] != '' else np.nan for line in lines]\n",
    "        Nui_h1_values = [line[5] if line[5] != '' else np.nan for line in lines]\n",
    "        Nui_h2_values = [line[6] if line[6] != '' else np.nan for line in lines]\n",
    "        Nui_h3_values = [line[7] if line[7] != '' else np.nan for line in lines]\n",
    "        Nui_h4_values = [line[8] if line[8] != '' else np.nan for line in lines]\n",
    "        Nuih1_values = [line[11] if line[11] != '' else np.nan for line in lines]\n",
    "        Nuih2_values = [line[12] if line[12] != '' else np.nan for line in lines]\n",
    "    Nuih1_contigs_values_uniq = list(set(Nuih1_contigs_values))\n",
    "    Nuih2_contigs_values_uniq = list(set(Nuih2_contigs_values))\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "except IndexError:\n",
    "    print(f\"Column index out of range: {column_index}\")\n",
    "\n",
    "\n",
    "for unique_value in Nuih1_contigs_values_uniq:\n",
    "    matching_indices = [index for index, value in enumerate(Nuih1_contigs_values) if value == unique_value]\n",
    "    print(f\"Indices of values equal to {unique_value}: {matching_indices}\")\n",
    "    corresponding_values_Nui_h1 = [Nui_h1_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nui_h1 Values: {corresponding_values_Nui_h1}\\n\")\n",
    "    corresponding_values_Nui_h2 = [Nui_h2_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nui_h2 Values: {corresponding_values_Nui_h2}\\n\")\n",
    "    corresponding_values_Nui_h3 = [Nui_h3_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nui_h3 Values: {corresponding_values_Nui_h3}\\n\")\n",
    "    corresponding_values_Nui_h4 = [Nui_h4_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nui_h4 Values: {corresponding_values_Nui_h4}\\n\")\n",
    "    corresponding_values_Nuih1 = [Nuih1_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nuih1 Values: {corresponding_values_Nuih1}\\n\")\n",
    "    corresponding_values_Nuih2 = [Nuih2_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nuih2 Values: {corresponding_values_Nuih2}\\n\")\n",
    "    \n",
    "    total_values_num = len(corresponding_values_Nuih1)\n",
    "\n",
    "    print(len(corresponding_values_Nuih1))\n",
    "\n",
    "    frequency_counts = Counter(corresponding_values_Nuih1)\n",
    "    for value, count in frequency_counts.items():\n",
    "        print(f\"Nuih1 value: {value}, Frequency: {count}\")\n",
    "\n",
    "     # Check if Nuih1 value is '1' before printing corresponding values\n",
    "    for index in matching_indices:\n",
    "        output_file_path = 'test_Nuih1.chr2.txt'\n",
    "        if Nuih1_values[index] == '1' and Nuih2_values[index] != '1':\n",
    "            print(f\"Index: {index}\")\n",
    "            corresponding_values_Nui_h1 = Nui_h1_values[index]\n",
    "            print(f\"Corresponding Nui_h1 Value: {corresponding_values_Nui_h1}\")\n",
    "            corresponding_values_Nui_h2 = Nui_h2_values[index]\n",
    "            print(f\"Corresponding Nui_h2 Value: {corresponding_values_Nui_h2}\")\n",
    "            corresponding_values_Nui_h3 = Nui_h3_values[index]\n",
    "            print(f\"Corresponding Nui_h3 Value: {corresponding_values_Nui_h3}\")\n",
    "            corresponding_values_Nui_h4 = Nui_h4_values[index]\n",
    "            print(f\"Corresponding Nui_h4 Value: {corresponding_values_Nui_h4}\")\n",
    "            corresponding_values_Nuih1 = Nuih1_values[index]\n",
    "            print(f\"Corresponding Nuih1 Value: {corresponding_values_Nuih1}\")\n",
    "            corresponding_values_Nuih2 = Nuih2_values[index]\n",
    "            print(f\"Corresponding Nuih2 Value: {corresponding_values_Nuih2}\\n\")\n",
    "            \n",
    "            corresponding_values_dict = {\n",
    "                \"corresponding_values_Nui_h1\": corresponding_values_Nui_h1,\n",
    "                \"corresponding_values_Nui_h2\": corresponding_values_Nui_h2,\n",
    "                \"corresponding_values_Nui_h3\": corresponding_values_Nui_h3,\n",
    "                \"corresponding_values_Nui_h4\": corresponding_values_Nui_h4,\n",
    "            }\n",
    "\n",
    "            print(f\"corresponding values dict: {corresponding_values_dict}\\n\")\n",
    "\n",
    "            largest_value = max(corresponding_values_dict.values())\n",
    "            print(f\"largest value: {largest_value}\\n\")\n",
    "        # Find all columns with the largest value\n",
    "            largest_columns = [column for column, variant in corresponding_values_dict.items() if variant == largest_value]\n",
    "            print(f\"largest columns: {largest_columns}\\n\")\n",
    "\n",
    "            if largest_value == '1' and len(largest_columns) < 2:\n",
    "                with open(output_file_path, 'a') as output_file:\n",
    "                    output_file.write(f\"{unique_value}\\t{marker_id[index]}\\t{largest_columns[0].split('_')[-1].split(']')[0]}\\n\")\n",
    "            else:\n",
    "                with open(output_file_path, 'a') as output_file:\n",
    "                    output_file.write(f\"{unique_value}\\t{marker_id[index]}\\tNaN\\n\")\n",
    "\n",
    "        else:\n",
    "            with open(output_file_path, 'a') as output_file:\n",
    "                output_file.write(f\"{unique_value}\\t{marker_id[index]}\\tNaN\\n\")      \n",
    "\n",
    "\n",
    "for unique_value in Nuih2_contigs_values_uniq:\n",
    "    matching_indices = [index for index, value in enumerate(Nuih2_contigs_values) if value == unique_value]\n",
    "    print(f\"Indices of values equal to {unique_value}: {matching_indices}\")\n",
    "    corresponding_values_Nui_h1 = [Nui_h1_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nui_h1 Values: {corresponding_values_Nui_h1}\\n\")\n",
    "    corresponding_values_Nui_h2 = [Nui_h2_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nui_h2 Values: {corresponding_values_Nui_h2}\\n\")\n",
    "    corresponding_values_Nui_h3 = [Nui_h3_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nui_h3 Values: {corresponding_values_Nui_h3}\\n\")\n",
    "    corresponding_values_Nui_h4 = [Nui_h4_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nui_h4 Values: {corresponding_values_Nui_h4}\\n\")\n",
    "    corresponding_values_Nuih1 = [Nuih1_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nuih1 Values: {corresponding_values_Nuih1}\\n\")\n",
    "    corresponding_values_Nuih2 = [Nuih2_values[index] for index in matching_indices]\n",
    "    print(f\"Corresponding Nuih2 Values: {corresponding_values_Nuih2}\\n\")\n",
    "    \n",
    "    total_values_num = len(corresponding_values_Nuih2)\n",
    "\n",
    "    print(len(corresponding_values_Nuih2))\n",
    "\n",
    "    frequency_counts = Counter(corresponding_values_Nuih2)\n",
    "    for value, count in frequency_counts.items():\n",
    "        print(f\"Nuih2 value: {value}, Frequency: {count}\")\n",
    "\n",
    "     # Check if Nuih2 value is '1' before printing corresponding values\n",
    "    for index in matching_indices:\n",
    "        output_file_path = 'test_Nuih2.chr2.txt'\n",
    "        if Nuih2_values[index] == '1' and Nuih1_values[index] != '1':\n",
    "            print(f\"Index: {index}\")\n",
    "            corresponding_values_Nui_h1 = Nui_h1_values[index]\n",
    "            print(f\"Corresponding Nui_h1 Value: {corresponding_values_Nui_h1}\")\n",
    "            corresponding_values_Nui_h2 = Nui_h2_values[index]\n",
    "            print(f\"Corresponding Nui_h2 Value: {corresponding_values_Nui_h2}\")\n",
    "            corresponding_values_Nui_h3 = Nui_h3_values[index]\n",
    "            print(f\"Corresponding Nui_h3 Value: {corresponding_values_Nui_h3}\")\n",
    "            corresponding_values_Nui_h4 = Nui_h4_values[index]\n",
    "            print(f\"Corresponding Nui_h4 Value: {corresponding_values_Nui_h4}\")\n",
    "            corresponding_values_Nuih1 = Nuih1_values[index]\n",
    "            print(f\"Corresponding Nuih1 Value: {corresponding_values_Nuih1}\")\n",
    "            corresponding_values_Nuih2 = Nuih2_values[index]\n",
    "            print(f\"Corresponding Nuih2 Value: {corresponding_values_Nuih2}\\n\")\n",
    "            \n",
    "            corresponding_values_dict = {\n",
    "                \"corresponding_values_Nui_h1\": corresponding_values_Nui_h1,\n",
    "                \"corresponding_values_Nui_h2\": corresponding_values_Nui_h2,\n",
    "                \"corresponding_values_Nui_h3\": corresponding_values_Nui_h3,\n",
    "                \"corresponding_values_Nui_h4\": corresponding_values_Nui_h4,\n",
    "            }\n",
    "\n",
    "            print(f\"corresponding values dict: {corresponding_values_dict}\\n\")\n",
    "\n",
    "            largest_value = max(corresponding_values_dict.values())\n",
    "            print(f\"largest value: {largest_value}\\n\")\n",
    "        # Find all columns with the largest value\n",
    "            largest_columns = [column for column, variant in corresponding_values_dict.items() if variant == largest_value]\n",
    "            print(f\"largest columns: {largest_columns}\\n\")\n",
    "\n",
    "            if largest_value == '1' and len(largest_columns) < 2:\n",
    "               \n",
    "                with open(output_file_path, 'a') as output_file:\n",
    "                    output_file.write(f\"{unique_value}\\t{marker_id[index]}\\t{largest_columns[0].split('_')[-1].split(']')[0]}\\n\")\n",
    "            else:\n",
    "                with open(output_file_path, 'a') as output_file:\n",
    "                    output_file.write(f\"{unique_value}\\t{marker_id[index]}\\tNaN\\n\")\n",
    "\n",
    "        else:\n",
    "            with open(output_file_path, 'a') as output_file:\n",
    "                output_file.write(f\"{unique_value}\\t{marker_id[index]}\\tNaN\\n\")\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4cf16-219b-4422-bc29-5d8c164af1f0",
   "metadata": {},
   "source": [
    "* formating the output files for plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73cc6ba5-a8bd-434a-a1db-ec50488d27b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP_hA\t229781\tlightgrey\n",
      "HP_hA\t229801\tlightgrey\n",
      "HP_hA\t229820\tlightgrey\n",
      "HP_hA\t229830\tlightgrey\n",
      "HP_hA\t247149\tlightgrey\n",
      "HP_hA\t247160\tlightgrey\n",
      "HP_hA\t252157\tlightgrey\n",
      "HP_hA\t268324\tlightgrey\n",
      "HP_hA\t297604\tlightgrey\n",
      "HP_hA\t299184\tlightgrey\n",
      "HP_hA\t372975\tlightgrey\n",
      "HP_hA\t373014\tlightgrey\n",
      "HP_hA\t373016\tlightgrey\n",
      "HP_hA\t373057\tlightgrey\n",
      "HP_hA\t386399\tlightgrey\n",
      "HP_hA\t428025\tblue\n",
      "HP_hA\t439038\tlightgrey\n",
      "HP_hA\t530031\tlightgrey\n",
      "HP_hA\t700408\tlightgrey\n",
      "HP_hA\t736734\tlightgrey\n"
     ]
    }
   ],
   "source": [
    "# giving colours to those variants able to be assigned with parental haplotype\n",
    "\n",
    "head -n 20 003.chr2PhasedLocus/chr2.markers_M7h1.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
